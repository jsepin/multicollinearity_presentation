\documentclass[english]{beamer}

%% The most common packages are already included in:
\usetheme{biostat}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%% Header data: (adjust to your needs:
\def\uzhunit{Biostatistics}             %% if (not) needed comment/uncomment
%\def\uzhunitext{STA480}

\title[Multicollinearity]{Multicollinearity}
%% Optional Argument in [Brackets]: Short Title for Footline

%% The following are all optional, simply comment them
%\subtitle{Subtitle (optional)}
%\institute{Biostatistics Journal Club}  %% optional
\author{Jerome Sepin}
%\date{\today}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
<<include=FALSE,purl=FALSE>>=
library(knitr)
opts_chunk$set(
fig.path='figures/fig', fig.show='hold', 
	cache=F, fig.height=4)
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{document}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%===================================================
\begin{frame}{Why this presentation?}
%===================================================
From \cite{Schwabe2022}:

\textit{
"The primary pharmacokinetic parameters were log-transformed and analyzed using an analysis of covariance (ANCOVA) model that included treatment as a fixed effect, \colorbox{yellow}{baseline weight} category $\left(\geq 60-\leq 80kg/>80\text{kg}-\leq 100\text{kg}\right)$, \colorbox{yellow}{baseline BMI} (continuous variable), and study site as covariates." 
}
\begin{center}
$\boxed{\log(y)\sim trt + weight_{cat.} + BMI_{cont.}+ site}$
\end{center}

%===================================================
\end{frame}
\begin{frame}{What is the problem with this?}
%===================================================
\begin{center}
$\boxed{\log(y)\sim trt + weight_{cat.} + BMI_{cont.}+ site}$
\end{center}
Since $\text{BMI}=\frac{\text{weight (kg)}}{\text{height (m)}^2}$
\begin{center}
$\boxed{\log(y)\sim trt + weight_{cat.} + \frac{weight_{cont.}}{height^2_{cont.}}+ site}$
\end{center}

Thus, $weight$ and $BMI$ may be highly correlated (a special case of collinearity)!

<<simdata, echo=F,eval = T, message=F, warning =F>>=
library(mvtnorm);library(tidyverse)
n <- round(685 * 2/3,0)
# bmi_m <- 24.5
# bmi_sd <- 2.6
height_m <- 170
height_sd <- 3#7
weight_m <- 73.5
weight_sd <- 5#9.4
beta <- c(5,2,2,2)

sigma <- 2
rho <- 0.5

dgp <- function(rho,n,sigma,beta, bmi_m, bmi_sd, weight_m, weight_sd,show_data = FALSE){

	# generate baseline covariates
	X <- data.frame(mvtnorm::rmvnorm(n=n,
												mean = c(height_m,weight_m),
												sigma = matrix(c(height_sd^2,rho*height_sd*weight_sd,
																				 rho*height_sd*weight_sd,weight_sd^2),nrow = 2, byrow = T)
												))
	
	eps_y <- rnorm(n = n, mean = 0, sd = sigma)
	
	
	colnames(X) <- c("height", "weight")
	
	# # recover height
	# X$height <- sqrt(X$weight/X$bmi)
	
	# recover bmi
	X$bmi <- X$weight/((X$height/100)^2)
	
	# categorical weight
	X$weight_cat <- ifelse(X$weight > 80, ">80kg", "<=80" )
	
	# treatment randomization 1:1:1 within categorical weight
	rand <- lapply(unique(X$weight_cat), function(x){
		S_ind <- which(X$weight_cat==x)
		Trt <- rep_len(c(0:1), length.out=length(S_ind))
		return(cbind(S_ind,Trt))
	})
	rand <- data.frame(do.call("rbind", rand))
	X$Trt <- rand$Trt[order(rand$S_ind)]
	
	#table(X$weight_cat,X$Trt)
	
	# outcome
	#X$Y_log <- cbind(1,X$Trt,X$bmi, ifelse(X$weight_cat=="<=80",0,1)) %*% beta +eps_y
	#X$Y_log <- cbind(1,X$Trt,X$bmi, X$weight) %*% beta + eps_y
	X$Y_log <- cbind(1,X$Trt,X$height, X$weight) %*% beta + eps_y
	
	
	#modelling
	#m <- lm(data = X, Y_log~Trt + bmi + weight_cat)
	m <- lm(data = X, Y_log~Trt + bmi + weight)
	s <- data.frame(coef(summary(m)))
	s$variable <- row.names(s)
	s$model <- "m2:bmi+weight"

	m <- lm(data = X, Y_log~Trt + height + weight)
	s2 <- data.frame(coef(summary(m)))
	s2$variable <- row.names(s2)
	s2$model <- "m1:height+weight"
	
	m <- lm(data = X, Y_log~Trt + height)
	s3 <- data.frame(coef(summary(m)))
	s3$variable <- row.names(s3)
	s3$model <- "m4:height"
	
	m <- lm(data = X, Y_log~Trt + weight)
	s4 <- data.frame(coef(summary(m)))
	s4$variable <- row.names(s4)
	s4$model <- "m5:weight"
	
	
	m <- lm(data = X, Y_log~Trt + bmi + height)
	s5 <- data.frame(coef(summary(m)))
	s5$variable <- row.names(s5)
	s5$model <- "m3:bmi+height"
	
	m <- lm(data = X, Y_log~Trt + bmi )
	s6 <- data.frame(coef(summary(m)))
	s6$variable <- row.names(s6)
	s6$model <- "m6:bmi"
	
	m <- lm(data = X, Y_log~Trt + bmi+height+weight )
	s7 <- data.frame(coef(summary(m)))
	s7$variable <- row.names(s7)
	s7$model <- "m7:bmi+height+weight"
	
	m <- lm(data = X, Y_log~Trt)
	s8 <- data.frame(coef(summary(m)))
	s8$variable <- row.names(s8)
	s8$model <- "m8:"
	
	
	s <- rbind(s,s2,s3,s4,s5,s6,s7,s8)
	
	
	s$rho <- rho
	s$rho_sim <- cor(X$height, X$weight)
	
	
	#give back
	if(show_data){
		return(X)
	}else{
		return(s)
	}
}

set.seed(324324)

rho <- rep(seq(from = 0.6, to =0.99, length.out = 50),5)
#rho <- rep(seq(from = 1, to = 5, length.out = 100),5)
#rho <- tanh(rep(seq(from = 1, to = 5, length.out = 50),5))

res <- lapply(rho, function(x){
	dgp(rho=x,n = 100,sigma = sigma,beta=beta,bmi_m=bmi_m, bmi_sd = bmi_sd, weight_m = weight_m, weight_sd = weight_sd)
})
res <- do.call("rbind", res)
row.names(res) <-NULL

res$lower <- res$Estimate - qnorm(p = 0.95)*res$Std..Error
res$upper <- res$Estimate + qnorm(p = 0.95)*res$Std..Error

# add true values
res <- left_join(res, data.frame("variable"=c("(Intercept)", "Trt", "height", "weight"),"true"=beta))


#fill missing combination
res <- as.data.frame(complete(res,variable,model,fill=list(NA) ))
#table(res$variable,res$model)
@


%===================================================
\end{frame}
\begin{frame}{What is the problem with collinear variables in the model?}
%===================================================

Let's see what happens when the correlation between $height$ and $weight$ changes!

\begin{figure}[h]%H is strict!
\begin{center}
<<collinearitydata, fig.height = 4, fig.width = 10,echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE,dev='pdf'>>=
# dev.off()
# dev.new()
set.seed(324324)
d1 <- dgp(rho=min(rho),n = 100,sigma = sigma,beta=beta,bmi_m=bmi_m, bmi_sd = bmi_sd, weight_m = weight_m, weight_sd = weight_sd, show_data = TRUE)
par(mar  = c(5,5,2,4) ,mfrow = c(1,2))
plot(d1$weight, d1$height,pch = 19,xlab = "weight [kg]", ylab = "height [m]", main = "Low correlation")
par(new = T, mar  =  c(5,5,2,4) )
plot(d1$weight, d1$bmi,col = "blue",pch = 19,xaxt = "n", xlab = "",ylab = "", yaxt = "n")
mtext("BMI",side=4,col="blue",line=4) 
axis(4, ylim=c(0,7000), col="blue",col.axis="blue",las=1)


d1 <- dgp(rho=max(rho),n = 100,sigma = sigma,beta=beta,bmi_m=bmi_m, bmi_sd = bmi_sd, weight_m = weight_m, weight_sd = weight_sd, show_data = TRUE)
par(mar  =c(5,5,2,5) )
plot(d1$weight, d1$height,pch = 19,xlab = "weight [kg]", ylab = "height [m]", main = "High correlation")
par(new = T, mar  = c(5,5,2,5) )
plot(d1$weight, d1$bmi,col = "blue",pch = 19,xaxt = "n", xlab = "",ylab = "", yaxt = "n")
mtext("BMI",side=4,col="blue",line=4) 
axis(4, ylim=c(0,7000), col="blue",col.axis="blue",las=1)

@
\end{center}
\end{figure}

%===================================================
\end{frame}
\begin{frame}{Small simulation:}
%===================================================

\begin{center}
$\boxed{\log(y)\sim trt + weight_{cat.} + BMI_{cont.}+ site}$

For simplicity without $site$ and with continuous variables!

"True" model: $\log(y)\sim \Sexpr{beta[1]} +  \Sexpr{beta[2]}\cdot trt +  \Sexpr{beta[3]}\cdot weight +  \Sexpr{beta[4]}\cdot height$





\textcolor{red}{Dichotomia is a problem for a different day\dots}
\end{center}


% %===================================================
% \end{frame}
% \begin{frame}{Relation of variables}
% %===================================================
% \begin{enumerate}
% \item paper: $AUC\sim trt + weight_{cat.} + BMI_{cont.}\textcolor{gray}{+ site}$
% 
% $AUC\sim trt + weight_{cat.} + \frac{weight_{cont.}}{height_{cont.}^2}\textcolor{gray}{+ site}$
% \item a1: $AUC\sim trt + weight_{cat.} + height_{cont.}\textcolor{gray}{+ site}$
% \item a2: $AUC\sim trt + weight_{cont.} + height_{cont.}\textcolor{gray}{+ site}$
% \end{enumerate}
% 
% Let's see what happens when the correlation between $height$ and $weight$ changes!

%===================================================
\end{frame}
\begin{frame}[plain]{Small simulation: Results}
%===================================================

\begin{figure}[h]%H is strict!
\begin{center}
<<results, fig.height = 8, fig.width = 11,echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE,dev='pdf'>>=

#change type
res$variable <- factor(res$variable,levels=c("bmi","weight","height", "Trt"))


frame_range <- range(c(res %>% filter(variable == "Trt")%>% dplyr::select(lower,upper)))
frame <- data.frame(variable = "Trt",
										model = rep(unique(res$model),each = 2),
										Estimate = rep(frame_range,2),
										rho_sim = 1
										)
#change type
frame$variable <- factor(frame$variable,levels=c("bmi","weight","height", "Trt"))
res %>%
	filter(variable != "(Intercept)")%>%
	ggplot(aes(x=rho_sim,y =Estimate,col = model ))+ #rho_sim
	geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.01,alpha = 0.5)+
	geom_point(alpha = 0.5,col = "black")+
	#geom_smooth(col = "black", se = F)+
	geom_hline(aes(yintercept = true), col = "red" )+
	facet_wrap(variable~model, scales = "free",
						 ncol = length(unique(res$model)))+
	labs(x = "Correlation between weight and height",
			 y = "Estimate and 90% Confidence Interval")+
	geom_point(data = frame, aes(x=rho_sim,y = Estimate,col = model ),alpha = 0)

#+coord_cartesian(ylim = c(-10, 10))

@
\end{center}
\end{figure}

%===================================================
\end{frame}
\begin{frame}{Summary of results}
%===================================================

Variance of least-squares estimator:
\begin{align*}
\text{Var}\left(\boldsymbol{\hat\beta}\right)=\underbrace{\hat{\sigma}}_{\substack{\text{Residual standard}\\\text{error (RSE)}}}\cdot \underbrace{\left(\boldsymbol{X}^\top\boldsymbol{X}\right)^{-1}}_{\text{Collinearity}}
\end{align*}

$\downarrow\text{Var}\left(\boldsymbol{\hat\beta}\right)$ means more power. How to achieve?
\begin{itemize}
\item $\downarrow$ RSE: Risk of biased results in terms of generalization
\item $\downarrow$ Collinearity: Risk of biased effect estimates
\end{itemize}


%===================================================
\end{frame}
\begin{frame}{Relation of variables}
%===================================================

\tikzstyle{block} = [draw=black, thick, text width=2cm, minimum height=1cm, align=center]
\tikzstyle{cloud} = [draw=black, ellipse, text width=1cm, minimum height=1cm, align=center] 
% \tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
%     minimum height=2em]
\tikzstyle{arrow} = [thick,->,>=stealth]

\begin{figure}[H]
\centering
\resizebox{8.5cm}{!}{%
\begin{tikzpicture}[decoration={crosses, shape size=5pt, segment length = 20pt}]
		\node[cloud] (T) {T};
    \node[cloud,below right=3cm and 3cm of T] (weight) {$\textit{weight}_0$};
    \node[cloud, right=1.5cm of weight] (height) {$\textit{height}_0$};
    \node[cloud, below right=1.5cm and 4.5cm of T] (BMI) {$\textit{BMI}_0$};
		\node[cloud, right=9cm of T] (Y) {$Y$};

    % edges
    \draw[red] (T) -- (Y);
    \draw[line width=1.5mm ] (weight) -- (BMI);
    \draw[line width=1.5mm ] (height) -- (BMI);
    \draw (height) -- (weight);
    \draw (height) -- (Y);
    \draw (BMI) -- (Y);
    \draw (weight) to [bend right=80] (Y);
    \draw[gray!80] (BMI) -- (T);
    \draw[gray!80] (weight) -- (T);
    \draw[gray!80] (height) to [bend left=80] (T);
    
    \draw [decorate, red] (BMI) -- (T);
    \draw [decorate, red] (weight) -- (T);
    \draw [decorate, red] (height) to [bend left=80] (T);

\end{tikzpicture}
}
\vspace*{-1.cm}
\caption{Randomization removes confounding}
\end{figure}
\vspace*{-0.8cm}
\begin{center}
$\boxed{\text{BMI}=\frac{\text{weight (kg)}}{\text{height (m)}^2}}$
\end{center}

%===================================================
\end{frame}
\begin{frame}{But why this model?}
%===================================================
\centering
\huge{\textbf{Domain knowledge?}}

\normalsize
\textcolor{red}{(Statistical) Model selection is a problem for a different day\dots}

%===================================================
\end{frame}
\begin{frame}[plain]{But why this model?}
%===================================================
\cite{ICHE9}:
\textit{
\tiny{
"In multicentre trials (see Glossary) the randomisation procedures should be organised centrally. It is advisable to have a separate random scheme for each centre, i.e. to stratify by centre or to allocate several whole blocks to each centre. More generally, stratification by important prognostic factors measured at baseline (e.g. severity of disease, age, sex, etc.) may sometimes be valuable in order to promote balanced allocation within strata; this has greater potential benefit in small trials.}\footnotesize{\cbox{The use of more than two or three stratification factors is rarely necessary, is less successful at achieving balance and is logistically troublesome.}}\tiny{The use of a dynamic allocation procedure (see below) may help to achieve balance across a number of stratification factors simultaneously provided the rest of the trial procedures can be adjusted to accommodate an approach of this type.}\footnotesize{\cbox{Factors on which randomisation has been stratified should be accounted for later in the analysis.}"}
}


%===================================================
\end{frame}
\begin{frame}{Collinearity != Correlation}
%===================================================

\begin{itemize}
\item Correlation is a special case of collinearity (includes only two variables)
\item Problems can also arise without large (pairwise) correlations
\item For example when explanatory variables add up to 100\% (recipe)
\end{itemize}


\begin{table}[]
\centering
%\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|l|}
\hline
milk & water & ... & salt & total\\ 
\hline
40\% & 30\%  & ... & 1\%  & 100\%\\ 
45\% & 35\%  & ... & 2\%  & 100\%\\
\dots& \dots  & \dots & \dots & 100\% \\ \hline
\end{tabular}%
%}
\end{table}

<<coupling, echo=F, eval = F>>=
n <- 100
set.seed(234324)
x1 <- round(runif(n,0,0.2),2)
x2 <- round(runif(n,0,0.4),2)
x3 <- round(runif(n,0,0.2),2)
x4 <- 1-x1-x2-x3
beta <- rpois(n=4+1,lambda = 5)
y <- cbind(1,x1,x2,x3,x4) %*% beta + rnorm(n = n, mean = 0, sd = 1)
d <- data.frame(x1,x2,x3,x4,y)
m <- lm(data = d, y~.)
summary(m)
confint(m)
beta

X <- model.matrix(m)
solve(t(X)%*%X)

cor(d[,-5])
solve(cor(d[,-5]))

@


%===================================================
\end{frame}
\begin{frame}{ }
%===================================================
Recipes remain secret!
\centering
\includegraphics[width=0.9\textwidth]{appenzeller.png}

%===================================================
\end{frame}
\begin{frame}{Conclusion}
%===================================================

\begin{itemize}
\item Collinearity increases uncertainty (but only for the variables that contribute)
\item "Killing" collinearity may induce bias
\item Randomization to the help
\item Trade-off between explanatory power and sparsity (bias-variance dilemma)
\end{itemize}


%===================================================
\end{frame}
\begin{frame}{References}
%===================================================
  \small
  \bibliographystyle{apalike}
\bibliography{illustration}

\end{frame}

%\appendix
%% Possible backup slides...

%% chapter division is accomplished with:
%% \part{Appendix}

\end{document}